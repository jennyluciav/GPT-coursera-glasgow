{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2: Evaluating an N-Gram Language Model\n",
    "\n",
    "In this lab, you will evaluate the quality of an n-gram language model using perplexity.\n",
    "\n",
    "We have built several n-gram language models and provided an implementation for computing the probabilities. The implementation includes [Laplace Smoothing](https://en.wikipedia.org/wiki/Additive_smoothing), with assigns some probability to sequences that were never encountered during training.\n",
    "\n",
    "First, review the implementation below to make sure that it makes sense to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "BOS = '<BOS>'\n",
    "EOS = '<EOS>'\n",
    "OOV = '<OOV>'\n",
    "class NGramLM:\n",
    "    def __init__(self, path, smoothing=0.001, verbose=False):\n",
    "        with open(path, 'rb') as fin:\n",
    "            data = pickle.load(fin)\n",
    "        self.n = data['n']\n",
    "        self.V = set(data['V'])\n",
    "        self.model = data['model']\n",
    "        self.smoothing = smoothing\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def get_prob(self, context, token):\n",
    "        # Take only the n-1 most recent context (Markov Assumption)\n",
    "        context = tuple(context[-self.n+1:])\n",
    "        # Add <BOS> tokens if the context is too short, i.e., it's at the start of the sequence\n",
    "        while len(context) < (self.n-1):\n",
    "            context = (BOS,) + context\n",
    "        # Handle words that were not encountered during the training by replacing them with a special <OOV> token\n",
    "        context = tuple((c if c in self.V else OOV) for c in context)\n",
    "        if token not in self.V:\n",
    "            token = OOV\n",
    "        if context in self.model:\n",
    "            # Compute the probability using a Maximum Likelihood Estimation and Laplace Smoothing\n",
    "            count = self.model[context].get(token, 0)\n",
    "            prob = (count + self.smoothing) / (sum(self.model[context].values()) + self.smoothing * len(self.V))\n",
    "        else:\n",
    "            # Simplified formula if we never encountered this context; the probability of all tokens is uniform\n",
    "            prob = 1 / len(self.V)\n",
    "        # Optional logging\n",
    "        if self.verbose:\n",
    "            print(f'{prob:.4n}', *context, '->', token)\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-built n-gram languae models\n",
    "model_unigram = NGramLM('arthur-conan-doyle.tok.train.n1.pkl')\n",
    "model_bigram = NGramLM('arthur-conan-doyle.tok.train.n2.pkl')\n",
    "model_trigram = NGramLM('arthur-conan-doyle.tok.train.n3.pkl')\n",
    "model_4gram = NGramLM('arthur-conan-doyle.tok.train.n4.pkl')\n",
    "model_5gram = NGramLM('arthur-conan-doyle.tok.train.n5.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to see how well these models fit our data! We'll use Perplexity for this calculation, but it's up to you to implement it below.\n",
    "\n",
    "Recall the formula for perplexity from the lecture:\n",
    "\n",
    "$$\n",
    "perplexity = 2^{\\frac{-1}{n}\\sum \\log_2(P(w_i|w_{<i}))}\n",
    "$$\n",
    "\n",
    "Hint: you'll want to use the [`math.log2`](https://docs.python.org/3/library/math.html#math.log2) function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "744.5701113236693"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List, Tuple\n",
    "import math\n",
    "\n",
    "def perplexity(model: NGramLM, texts: List[Tuple[str]]) -> float:\n",
    "    log_prob_sum = 0\n",
    "    total_tokens = 0  # Total number of tokens\n",
    "\n",
    "    for text in texts:\n",
    "\n",
    "        for i in range(model.n - 1, len(text)):\n",
    "            context = text[i - model.n + 1:i]\n",
    "            token = text[i]\n",
    "            prob = model.get_prob(context, token)\n",
    "            log_prob_sum += math.log2(prob)\n",
    "            total_tokens += 1\n",
    "            #print(f\"Context: {context}, Token: {token}, Probability: {prob}, Log Prob Sum: {log_prob_sum}\")\n",
    "\n",
    "    perplexity_value = 2 ** (-log_prob_sum / total_tokens)\n",
    "    return perplexity_value\n",
    "\n",
    "# Example:\n",
    "perplexity(model_unigram, [('My', 'dear', 'Watson', '.'), ('Come', 'over', 'here', '!')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-eb75f16ffc5b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Tests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperplexity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_unigram\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'My'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dear'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Watson'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m7531\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperplexity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_bigram\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'My'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dear'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Watson'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m24\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperplexity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_trigram\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'My'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dear'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Watson'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m521\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Tests\n",
    "assert round(perplexity(model_unigram, [('My', 'dear', 'Watson')])) == 7531\n",
    "assert round(perplexity(model_bigram, [('My', 'dear', 'Watson')])) == 24\n",
    "assert round(perplexity(model_trigram, [('My', 'dear', 'Watson')])) == 521"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now let's see how well the model fits a held-out test set.\n",
    "\n",
    "The test data covers a few of the stories, and represents about 12% of the total data.\n",
    "\n",
    "Your task it to print the perplexity for the unigram, bigram, trigram, 4-gram, and 5-gram models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for unigram model: 621\n",
      "Perplexity for bigram model: 289\n",
      "Perplexity for trigram model: 1258\n",
      "Perplexity for 4-gram model: 5454\n",
      "Perplexity for 5-gram model: 11273\n"
     ]
    }
   ],
   "source": [
    "toks_test = []\n",
    "with open('arthur-conan-doyle.tok.test.txt', 'rt') as fin:\n",
    "    for line in fin:\n",
    "        toks_test.append(list(line.split()))\n",
    "    print(\"Perplexity for unigram model:\", round(perplexity(model_unigram, toks_test)))\n",
    "    print(\"Perplexity for bigram model:\", round(perplexity(model_bigram, toks_test)))\n",
    "    print(\"Perplexity for trigram model:\", round(perplexity(model_trigram, toks_test)))\n",
    "    print(\"Perplexity for 4-gram model:\", round(perplexity(model_4gram, toks_test)))\n",
    "    print(\"Perplexity for 5-gram model:\", round(perplexity(model_5gram, toks_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see that the perplexity for the bigram model is lower than the others. What does this indicate?\n",
    "\n",
    "The lecture mentioned that it's a bad idea to determine the quality of a model based on the perplexity of data that was used for training. Below, evaluate the same five models using the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for unigram model: 529\n",
      "Perplexity for bigram model: 60\n",
      "Perplexity for trigram model: 22\n",
      "Perplexity for 4-gram model: 17\n",
      "Perplexity for 5-gram model: 17\n"
     ]
    }
   ],
   "source": [
    "toks_train = []\n",
    "with open('arthur-conan-doyle.tok.train.txt', 'rt') as fin:\n",
    "    for line in fin:\n",
    "        toks_train.append(list(line.split()))\n",
    "    print(\"Perplexity for unigram model:\", round(perplexity(model_unigram, toks_train)))\n",
    "    print(\"Perplexity for bigram model:\", round(perplexity(model_bigram, toks_train)))\n",
    "    print(\"Perplexity for trigram model:\", round(perplexity(model_trigram, toks_train)))\n",
    "    print(\"Perplexity for 4-gram model:\", round(perplexity(model_4gram, toks_train)))\n",
    "    print(\"Perplexity for 5-gram model:\", round(perplexity(model_5gram, toks_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see that you get much lower perplexities when measuring on the training data, especially for the models with larger values of `n`. This suggests that the model is over-fitting to the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional Extras:\n",
    " - In the models we explore above, we use smoothing. What happens to perplexity calculations when smoothing isn't applied? You can try this out by setting `smoothing=0`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When smoothing is not applied (i.e., setting smoothing=0), any unseen n-gram in the training data will have a probability of zero. This will have a significant impact on the perplexity calculations because the log probability of zero is negative infinity. Consequently, the overall perplexity will also be undefined or extremely large.\n",
    "\n",
    "To demonstrate this, we can modify the get_prob method to remove smoothing and then observe the effect on perplexity calculations. Here's the modified version of the NGramLM class and the perplexity function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: (), Token: My, Probability: 0.0006605441154985051, Log Prob Sum: -10.564057462156757\n",
      "Context: (), Token: dear, Probability: 0.00040743842638225544, Log Prob Sum: -21.825187791356207\n",
      "Context: (), Token: Watson, Probability: 0.0015289229838485645, Log Prob Sum: -31.178456340096673\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "BOS = '<BOS>'\n",
    "EOS = '<EOS>'\n",
    "OOV = '<OOV>'\n",
    "\n",
    "class NGramLM:\n",
    "    def __init__(self, path, smoothing=0.0, verbose=False):\n",
    "        with open(path, 'rb') as fin:\n",
    "            data = pickle.load(fin)\n",
    "        self.n = data['n']\n",
    "        self.V = set(data['V'])\n",
    "        self.model = data['model']\n",
    "        self.smoothing = smoothing\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def get_prob(self, context, token):\n",
    "        # Ensure context is a tuple and contains only the n-1 most recent tokens\n",
    "        context = tuple(context[-self.n + 1:])\n",
    "        \n",
    "        # Pad context with <BOS> tokens if it is too short\n",
    "        while len(context) < (self.n - 1):\n",
    "            context = (BOS,) + context\n",
    "        \n",
    "        # Replace OOV tokens in context with <OOV>\n",
    "        context = tuple((c if c in self.V else OOV) for c in context)\n",
    "        \n",
    "        # Replace token with <OOV> if it's not in the vocabulary\n",
    "        if token not in self.V:\n",
    "            token = OOV\n",
    "        \n",
    "        # Retrieve the context from the model\n",
    "        if context in self.model:\n",
    "            count = self.model[context].get(token, 0)\n",
    "            prob = count / sum(self.model[context].values()) if sum(self.model[context].values()) > 0 else 0\n",
    "        else:\n",
    "            # Handle unseen context: probability is zero\n",
    "            prob = 0\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f'Context: {context} Token: {token} Count: {count if context in self.model else \"N/A\"} Prob: {prob:.4f}')\n",
    "        \n",
    "        return prob\n",
    "\n",
    "# Load pre-built n-gram language models with smoothing=0\n",
    "model_unigram_no_smoothing = NGramLM('arthur-conan-doyle.tok.train.n1.pkl', smoothing=0.0)\n",
    "model_bigram_no_smoothing = NGramLM('arthur-conan-doyle.tok.train.n2.pkl', smoothing=0.0)\n",
    "model_trigram_no_smoothing = NGramLM('arthur-conan-doyle.tok.train.n3.pkl', smoothing=0.0)\n",
    "model_4gram_no_smoothing = NGramLM('arthur-conan-doyle.tok.train.n4.pkl', smoothing=0.0)\n",
    "model_5gram_no_smoothing = NGramLM('arthur-conan-doyle.tok.train.n5.pkl', smoothing=0.0)\n",
    "\n",
    "# Define the perplexity function\n",
    "from typing import List, Tuple\n",
    "import math\n",
    "\n",
    "def perplexity(model: NGramLM, texts: List[Tuple[str]]) -> float:\n",
    "    log_prob_sum = 0\n",
    "    total_tokens = 0  # Total number of tokens\n",
    "\n",
    "    for text in texts:\n",
    "        # Add BOS and EOS tokens to the text\n",
    "        # text = (BOS,) * (model.n - 1) + text + (EOS,)\n",
    "\n",
    "        for i in range(model.n - 1, len(text)):\n",
    "            context = text[i - model.n + 1:i]\n",
    "            token = text[i]\n",
    "            prob = model.get_prob(context, token)\n",
    "            if prob > 0:\n",
    "                log_prob_sum += math.log2(prob)\n",
    "            else:\n",
    "                log_prob_sum += float('-inf')  # Log(0) is negative infinity\n",
    "            total_tokens += 1\n",
    "            print(f\"Context: {context}, Token: {token}, Probability: {prob}, Log Prob Sum: {log_prob_sum}\")\n",
    "\n",
    "    perplexity_value = 2 ** (-log_prob_sum / total_tokens)\n",
    "    return perplexity_value\n",
    "\n",
    "# Test the perplexity function without smoothing\n",
    "try:\n",
    "    round(perplexity(model_unigram_no_smoothing, [('My', 'dear', 'Watson')])) == 7531\n",
    "except ValueError as e:\n",
    "    print(f\"Error in perplexity calculation: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When smoothing is set to zero, any unseen n-gram in the data will lead to a zero probability, which will result in the log probability being negative infinity. This will cause the overall perplexity to be undefined or extremely high, reflecting the poor generalization of the model to unseen data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Interpolated or \"back-off\" smoothing is sometimes used in n-gram language models. This techniques computes the weighted average probability of models with different values of `n`. Try implementing this yourself. How does it affect the perplexity on the held-out test set? What about the perplexity on the training data?\n",
    "\n",
    "Interpolated smoothing is a technique used to compute the weighted average probability of models with different values of n. This method combines probabilities from various n-gram models, often leading to better performance on both the held-out test set and the training data. \n",
    "\n",
    "#### Implementation Steps\n",
    "- Define Interpolated NGramLM: Create a new class that combines multiple n-gram models using interpolated smoothing.\n",
    "- Probability Calculation: Compute the weighted average probability from the different n-gram models.\n",
    "- Perplexity Calculation: Use the interpolated model to compute perplexity on both training and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Perplexity: 257.7859942614262\n",
      "Test Perplexity: 1463.5311769137288\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from typing import List, Tuple\n",
    "import math\n",
    "\n",
    "BOS = '<BOS>'\n",
    "EOS = '<EOS>'\n",
    "OOV = '<OOV>'\n",
    "\n",
    "class NGramLM:\n",
    "    def __init__(self, path, smoothing=0.001, verbose=False):\n",
    "        with open(path, 'rb') as fin:\n",
    "            data = pickle.load(fin)\n",
    "        self.n = data['n']\n",
    "        self.V = set(data['V'])\n",
    "        self.model = data['model']\n",
    "        self.smoothing = smoothing\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def get_prob(self, context, token):\n",
    "        context = tuple(context[-self.n + 1:])\n",
    "        while len(context) < (self.n - 1):\n",
    "            context = (BOS,) + context\n",
    "        context = tuple((c if c in self.V else OOV) for c in context)\n",
    "        if token not in self.V:\n",
    "            token = OOV\n",
    "\n",
    "        if context in self.model:\n",
    "            count = self.model[context].get(token, 0)\n",
    "            total_count = sum(self.model[context].values())\n",
    "            if total_count == 0:\n",
    "                prob = 0\n",
    "            else:\n",
    "                prob = (count + self.smoothing) / (total_count + self.smoothing * len(self.V))\n",
    "        else:\n",
    "            prob = self.smoothing / len(self.V)\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f'Context: {context} Token: {token} Count: {count if context in self.model else \"N/A\"} Prob: {prob:.4f}')\n",
    "        \n",
    "        return prob\n",
    "\n",
    "# Load pre-built n-gram language models\n",
    "model_unigram = NGramLM('arthur-conan-doyle.tok.train.n1.pkl')\n",
    "model_bigram = NGramLM('arthur-conan-doyle.tok.train.n2.pkl')\n",
    "model_trigram = NGramLM('arthur-conan-doyle.tok.train.n3.pkl')\n",
    "model_4gram = NGramLM('arthur-conan-doyle.tok.train.n4.pkl')\n",
    "model_5gram = NGramLM('arthur-conan-doyle.tok.train.n5.pkl')\n",
    "\n",
    "class InterpolatedNGramLM:\n",
    "    def __init__(self, models, lambdas):\n",
    "        assert len(models) == len(lambdas), \"Number of models must match number of lambdas\"\n",
    "        assert abs(sum(lambdas) - 1.0) < 1e-5, \"Lambdas must sum to 1\"\n",
    "        self.models = models\n",
    "        self.lambdas = lambdas\n",
    "        self.n = max(model.n for model in models)\n",
    "    \n",
    "    def get_prob(self, context, token):\n",
    "        prob = 0.0\n",
    "        for model, lambda_ in zip(self.models, self.lambdas):\n",
    "            prob += lambda_ * model.get_prob(context, token)\n",
    "        return prob\n",
    "\n",
    "def perplexity(model: NGramLM, texts: List[Tuple[str]]) -> float:\n",
    "    log_prob_sum = 0\n",
    "    total_tokens = 0  # Total number of tokens\n",
    "\n",
    "    for text in texts:\n",
    "        # To avoid error 'ZeroDivisionError'\n",
    "        text = (BOS,) * (model.n - 1) + text + (EOS,)\n",
    "        for i in range(model.n - 1, len(text)):\n",
    "            context = text[i - model.n + 1:i]\n",
    "            token = text[i]\n",
    "            prob = model.get_prob(context, token)\n",
    "            if prob > 0:\n",
    "                log_prob_sum += math.log2(prob)\n",
    "            else:\n",
    "                log_prob_sum += float('-inf')\n",
    "            total_tokens += 1\n",
    "\n",
    "    perplexity_value = 2 ** (-log_prob_sum / total_tokens)\n",
    "    return perplexity_value\n",
    "\n",
    "# Define lambdas for the interpolation\n",
    "lambdas = [0.1, 0.2, 0.3, 0.2, 0.2]\n",
    "\n",
    "# Create the interpolated model\n",
    "interpolated_model = InterpolatedNGramLM(\n",
    "    models=[model_unigram, model_bigram, model_trigram, model_4gram, model_5gram],\n",
    "    lambdas=lambdas\n",
    ")\n",
    "\n",
    "# Test the perplexity function on both training and test data\n",
    "train_texts = [('My', 'dear', 'Watson'), ('I', 'am', 'Sherlock', 'Holmes')]  # Add more training data\n",
    "test_texts = [('Sherlock', 'Holmes', 'said'), ('My', 'name', 'is', 'Watson')]  # Add more test data\n",
    "\n",
    "# Calculate perplexity on the training data\n",
    "train_perplexity = perplexity(interpolated_model, train_texts)\n",
    "print(f\"Training Perplexity: {train_perplexity}\")\n",
    "\n",
    "# Calculate perplexity on the test data\n",
    "test_perplexity = perplexity(interpolated_model, test_texts)\n",
    "print(f\"Test Perplexity: {test_perplexity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
