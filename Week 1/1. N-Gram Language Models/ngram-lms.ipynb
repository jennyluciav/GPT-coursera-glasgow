{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1: Building your own n-gram language model\n",
    "\n",
    "In this lab, you will build your own language model based on n-grams and the Maximum Likelihood Estimation.\n",
    "\n",
    "We'll assume that our text is already \"tokenized\" (split up into words). We'll cover this process in more depth in the next module.\n",
    "\n",
    "As an example, let's work with two sentences from \"[The Disappearance of Lady Frances Carfax](https://en.wikipedia.org/wiki/The_Disappearance_of_Lady_Frances_Carfax)\", a short story written by [Sir Arthur Conan Doyle](https://en.wikipedia.org/wiki/Arthur_Conan_Doyle)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokens for the sentence \"It shows, my dear Watson, that we are dealing\n",
    "# with an exceptionally astude and dangerous man.\"\n",
    "sample1 = ['It', 'shows', ',', 'my', 'dear', 'Watson', ',', 'that',\n",
    "           'we', 'are', 'dealing', 'with', 'an', 'exceptionally',\n",
    "           'astute', 'and', 'dangerous', 'man', '.']\n",
    "# Tokens for the sentence \"How would Lausanne do, my dear Watson?\"\n",
    "sample2 = ['How', 'would', 'Lausanne', 'do', ',', 'my', 'dear',\n",
    "           'Watson', '?']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your first task is to write a function that splits the `tokens` sequence\n",
    "into its `n`-grams.\n",
    "\n",
    "For instance, when `tokens=sample1` and `n=3`, your function should\n",
    "return:\n",
    "\n",
    "```python\n",
    "[('It', 'shows', ','),\n",
    " ('shows', ',', 'my'),\n",
    " (',', 'my', 'dear'),\n",
    " ...,\n",
    " ('dangerous', 'man', '.')]\n",
    "```\n",
    " \n",
    "Note: You should return a python [`list`](https://docs.python.org/3/tutorial/datastructures.html#more-on-lists) containing [`tuple`](https://docs.python.org/3/tutorial/datastructures.html#tuples-and-sequences)s. `tuple`s are immutable sequences, which will be useful later on when you build your language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('It', 'shows', ','),\n",
       " ('shows', ',', 'my'),\n",
       " (',', 'my', 'dear'),\n",
       " ('my', 'dear', 'Watson'),\n",
       " ('dear', 'Watson', ','),\n",
       " ('Watson', ',', 'that'),\n",
       " (',', 'that', 'we'),\n",
       " ('that', 'we', 'are'),\n",
       " ('we', 'are', 'dealing'),\n",
       " ('are', 'dealing', 'with'),\n",
       " ('dealing', 'with', 'an'),\n",
       " ('with', 'an', 'exceptionally'),\n",
       " ('an', 'exceptionally', 'astute'),\n",
       " ('exceptionally', 'astute', 'and'),\n",
       " ('astute', 'and', 'dangerous'),\n",
       " ('and', 'dangerous', 'man'),\n",
       " ('dangerous', 'man', '.')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List, Tuple\n",
    "def build_ngrams(tokens: List[str], n: int) -> List[Tuple[str]]:\n",
    "    ngrams = []\n",
    "    for i in range(len(tokens) - n + 1):\n",
    "        ngram = tuple(tokens[i:i + n])\n",
    "        ngrams.append(ngram)\n",
    "    return ngrams\n",
    "\n",
    "# Example:\n",
    "build_ngrams(sample1, n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests:\n",
    "assert len(build_ngrams(sample1, n=3)) == 17\n",
    "assert build_ngrams(sample1, n=3)[0] == ('It', 'shows', ',')\n",
    "assert build_ngrams(sample1, n=3)[10] == ('dealing', 'with', 'an')\n",
    "assert len(build_ngrams(sample1, n=2)) == 18\n",
    "assert build_ngrams(sample1, n=2)[0] == ('It', 'shows')\n",
    "assert build_ngrams(sample1, n=2)[10] == ('dealing', 'with')\n",
    "assert len(build_ngrams(sample2, n=2)) == 8\n",
    "assert build_ngrams(sample2, n=2)[0] == ('How', 'would')\n",
    "assert build_ngrams(sample2, n=2)[7] == ('Watson', '?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the current function, there's no way to know whether an n-gram is at the beginning, middle, or end of the sequence. To overcome this problem, n-gram language models often include special \"beginning-of-string\" (BOS) and \"end-of-string\" (EOS) control tokens.\n",
    "\n",
    "Write a new version of your `build_ngrams` function that includes these control tokens. For instance, when `tokens=sample1` and `n=3`, your new function should return:\n",
    "\n",
    "```python\n",
    "[('<BOS>', '<BOS>', 'It'),\n",
    " ('<BOS>', 'It', 'shows'),\n",
    " ('It', 'shows', ','),\n",
    " ('shows', ',', 'my'),\n",
    " (',', 'my', 'dear'),\n",
    " ...,\n",
    " ('dangerous', 'man', '.'),\n",
    " ('man', '.', '<EOS>'),\n",
    " ('.', '<EOS>', '<EOS>')]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<BOS>', '<BOS>', 'It'),\n",
       " ('<BOS>', 'It', 'shows'),\n",
       " ('It', 'shows', ','),\n",
       " ('shows', ',', 'my'),\n",
       " (',', 'my', 'dear'),\n",
       " ('my', 'dear', 'Watson'),\n",
       " ('dear', 'Watson', ','),\n",
       " ('Watson', ',', 'that'),\n",
       " (',', 'that', 'we'),\n",
       " ('that', 'we', 'are'),\n",
       " ('we', 'are', 'dealing'),\n",
       " ('are', 'dealing', 'with'),\n",
       " ('dealing', 'with', 'an'),\n",
       " ('with', 'an', 'exceptionally'),\n",
       " ('an', 'exceptionally', 'astute'),\n",
       " ('exceptionally', 'astute', 'and'),\n",
       " ('astute', 'and', 'dangerous'),\n",
       " ('and', 'dangerous', 'man'),\n",
       " ('dangerous', 'man', '.'),\n",
       " ('man', '.', '<EOS>'),\n",
       " ('.', '<EOS>', '<EOS>')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BOS = '<BOS>'\n",
    "EOS = '<EOS>'\n",
    "def build_ngrams_ctrl(tokens: List[str], n: int) -> List[Tuple[str]]:\n",
    "    ngrams = []\n",
    "    tokens = [BOS]*(n-1) + tokens + [EOS]*(n-1)\n",
    "    for i in range(len(tokens) - n + 1):\n",
    "        ngram = tuple(tokens[i:i + n])\n",
    "        ngrams.append(ngram)\n",
    "    return ngrams\n",
    "\n",
    "# Example:\n",
    "build_ngrams_ctrl(sample1, n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests:\n",
    "assert len(build_ngrams_ctrl(sample1, n=3)) == 21\n",
    "assert build_ngrams_ctrl(sample1, n=3)[0] == ('<BOS>', '<BOS>', 'It')\n",
    "assert build_ngrams_ctrl(sample1, n=3)[10] == ('we', 'are', 'dealing')\n",
    "assert len(build_ngrams_ctrl(sample1, n=2)) == 20\n",
    "assert build_ngrams_ctrl(sample1, n=2)[0] == ('<BOS>', 'It')\n",
    "assert build_ngrams_ctrl(sample1, n=2)[10] == ('are', 'dealing')\n",
    "assert len(build_ngrams_ctrl(sample2, n=2)) == 10\n",
    "assert build_ngrams_ctrl(sample2, n=2)[0] == ('<BOS>', 'How')\n",
    "assert build_ngrams_ctrl(sample2, n=2)[9] == ('?', '<EOS>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you can build n-grams, we have almost everything we need to build an n-gram language model.\n",
    "\n",
    "To compute Maximum Likelihood Estimations, you first need to count the number of times each word follows an n-gram of size `n-1`. You can build this structure as a Python [`dict`](https://docs.python.org/3/tutorial/datastructures.html#dictionaries) that maps the n-grams of size `n-1` to another `dict` that maps the following words to their respective counts.\n",
    "\n",
    "For instance, when `texts=[sample1, sample2]` and `n=3`, your function should return:\n",
    "\n",
    "```python\n",
    "{\n",
    "    ('<BOS>', '<BOS>'): {'It': 1, 'How': 1},\n",
    "    ('<BOS>', 'It'): {'shows': 1},\n",
    "    ('<BOS>', 'How'): {'would': 1},\n",
    "    ...\n",
    "    ('my', 'dear'): {'Watson': 2},\n",
    "    ('dear', 'Watson'): {',': 1, '?': 1},\n",
    "    ...\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.count_ngrams.<locals>.<lambda>()>,\n",
       "            {('<BOS>', '<BOS>'): defaultdict(int, {'It': 1, 'How': 1}),\n",
       "             ('<BOS>', 'It'): defaultdict(int, {'shows': 1}),\n",
       "             ('It', 'shows'): defaultdict(int, {',': 1}),\n",
       "             ('shows', ','): defaultdict(int, {'my': 1}),\n",
       "             (',', 'my'): defaultdict(int, {'dear': 2}),\n",
       "             ('my', 'dear'): defaultdict(int, {'Watson': 2}),\n",
       "             ('dear', 'Watson'): defaultdict(int, {',': 1, '?': 1}),\n",
       "             ('Watson', ','): defaultdict(int, {'that': 1}),\n",
       "             (',', 'that'): defaultdict(int, {'we': 1}),\n",
       "             ('that', 'we'): defaultdict(int, {'are': 1}),\n",
       "             ('we', 'are'): defaultdict(int, {'dealing': 1}),\n",
       "             ('are', 'dealing'): defaultdict(int, {'with': 1}),\n",
       "             ('dealing', 'with'): defaultdict(int, {'an': 1}),\n",
       "             ('with', 'an'): defaultdict(int, {'exceptionally': 1}),\n",
       "             ('an', 'exceptionally'): defaultdict(int, {'astute': 1}),\n",
       "             ('exceptionally', 'astute'): defaultdict(int, {'and': 1}),\n",
       "             ('astute', 'and'): defaultdict(int, {'dangerous': 1}),\n",
       "             ('and', 'dangerous'): defaultdict(int, {'man': 1}),\n",
       "             ('dangerous', 'man'): defaultdict(int, {'.': 1}),\n",
       "             ('man', '.'): defaultdict(int, {'<EOS>': 1}),\n",
       "             ('.', '<EOS>'): defaultdict(int, {'<EOS>': 1}),\n",
       "             ('<BOS>', 'How'): defaultdict(int, {'would': 1}),\n",
       "             ('How', 'would'): defaultdict(int, {'Lausanne': 1}),\n",
       "             ('would', 'Lausanne'): defaultdict(int, {'do': 1}),\n",
       "             ('Lausanne', 'do'): defaultdict(int, {',': 1}),\n",
       "             ('do', ','): defaultdict(int, {'my': 1}),\n",
       "             ('Watson', '?'): defaultdict(int, {'<EOS>': 1}),\n",
       "             ('?', '<EOS>'): defaultdict(int, {'<EOS>': 1})})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Dict\n",
    "from collections import defaultdict\n",
    "\n",
    "def count_ngrams(texts: List[List[str]], n: int) -> Dict[Tuple[str, ...], Dict[str, int]]:\n",
    "    ngram_counts = defaultdict(lambda: defaultdict(int))\n",
    "    \n",
    "    for tokens in texts:\n",
    "        ngrams = build_ngrams_ctrl(tokens, n)\n",
    "        for ngram in ngrams:\n",
    "            prefix = ngram[:-1]\n",
    "            suffix = ngram[-1]\n",
    "            ngram_counts[prefix][suffix] += 1\n",
    "    \n",
    "    return ngram_counts\n",
    "\n",
    "# Example:\n",
    "count_ngrams([sample1, sample2], n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests:\n",
    "assert len(count_ngrams([sample1, sample2], n=3)) == 28\n",
    "assert len(count_ngrams([sample1, sample2], n=3)['<BOS>', '<BOS>']) == 2\n",
    "assert count_ngrams([sample1, sample2], n=3)['<BOS>', '<BOS>']['It'] == 1\n",
    "assert count_ngrams([sample1, sample2], n=3)['<BOS>', '<BOS>']['How'] == 1\n",
    "assert count_ngrams([sample1, sample2], n=3)['my', 'dear']['Watson'] == 2\n",
    "assert len(count_ngrams([sample1, sample2], n=2)) == 24\n",
    "assert len(count_ngrams([sample1, sample2], n=2)['<BOS>',]) == 2\n",
    "assert count_ngrams([sample1, sample2], n=2)['<BOS>',]['It'] == 1\n",
    "assert count_ngrams([sample1, sample2], n=2)['<BOS>',]['How'] == 1\n",
    "assert count_ngrams([sample1, sample2], n=2)['dear',]['Watson'] == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You're almost there! The last step is to convert the counts into probability estimates.\n",
    "\n",
    "When `texts=[sample1, sample2]` and `n=3`, your function should return:\n",
    "\n",
    "```python\n",
    "{\n",
    "    ('<BOS>', '<BOS>'): {'It': 0.5, 'How': 0.5},\n",
    "    ('<BOS>', 'It'): {'shows': 1.0},\n",
    "    ('<BOS>', 'How'): {'would': 1.0},\n",
    "    ...\n",
    "    ('my', 'dear'): {'Watson': 1.0},\n",
    "    ('dear', 'Watson'): {',': 0.5, '?': 0.5},\n",
    "    ...\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('<BOS>', '<BOS>'): {'It': 0.5, 'How': 0.5},\n",
       " ('<BOS>', 'It'): {'shows': 1.0},\n",
       " ('It', 'shows'): {',': 1.0},\n",
       " ('shows', ','): {'my': 1.0},\n",
       " (',', 'my'): {'dear': 1.0},\n",
       " ('my', 'dear'): {'Watson': 1.0},\n",
       " ('dear', 'Watson'): {',': 0.5, '?': 0.5},\n",
       " ('Watson', ','): {'that': 1.0},\n",
       " (',', 'that'): {'we': 1.0},\n",
       " ('that', 'we'): {'are': 1.0},\n",
       " ('we', 'are'): {'dealing': 1.0},\n",
       " ('are', 'dealing'): {'with': 1.0},\n",
       " ('dealing', 'with'): {'an': 1.0},\n",
       " ('with', 'an'): {'exceptionally': 1.0},\n",
       " ('an', 'exceptionally'): {'astute': 1.0},\n",
       " ('exceptionally', 'astute'): {'and': 1.0},\n",
       " ('astute', 'and'): {'dangerous': 1.0},\n",
       " ('and', 'dangerous'): {'man': 1.0},\n",
       " ('dangerous', 'man'): {'.': 1.0},\n",
       " ('man', '.'): {'<EOS>': 1.0},\n",
       " ('.', '<EOS>'): {'<EOS>': 1.0},\n",
       " ('<BOS>', 'How'): {'would': 1.0},\n",
       " ('How', 'would'): {'Lausanne': 1.0},\n",
       " ('would', 'Lausanne'): {'do': 1.0},\n",
       " ('Lausanne', 'do'): {',': 1.0},\n",
       " ('do', ','): {'my': 1.0},\n",
       " ('Watson', '?'): {'<EOS>': 1.0},\n",
       " ('?', '<EOS>'): {'<EOS>': 1.0}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Dict\n",
    "def build_ngram_model(texts: List[List[str]], n: int) -> Dict[Tuple[str, ...], Dict[str, float]]:\n",
    "    ngram_counts = count_ngrams(texts, n)\n",
    "    ngram_model = {}\n",
    "    \n",
    "    for prefix, suffix_counts in ngram_counts.items():\n",
    "        total_count = sum(suffix_counts.values())\n",
    "        ngram_model[prefix] = {suffix: count / total_count for suffix, count in suffix_counts.items()}\n",
    "    \n",
    "    return ngram_model\n",
    "\n",
    "# Example:\n",
    "build_ngram_model([sample1, sample2], n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests:\n",
    "assert build_ngram_model([sample1, sample2], n=3)['<BOS>', '<BOS>']['It'] == 0.5\n",
    "assert build_ngram_model([sample1, sample2], n=3)['<BOS>', '<BOS>']['How'] == 0.5\n",
    "assert build_ngram_model([sample1, sample2], n=3)['my', 'dear']['Watson'] == 1.0\n",
    "assert build_ngram_model([sample1, sample2], n=2)['<BOS>',]['It'] == 0.5\n",
    "assert build_ngram_model([sample1, sample2], n=2)['<BOS>',]['How'] == 0.5\n",
    "assert build_ngram_model([sample1, sample2], n=2)['dear',]['Watson'] == 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A language model built from only a few sentences is not very informative. Let's scale up and see what your language model looks like when we train on the complete works of Sir Arthur Conon Doyle!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text = []\n",
    "with open('arthur-conan-doyle.tok.train.txt', 'rt') as fin:\n",
    "    for line in fin:\n",
    "        full_text.append(list(line.split()))\n",
    "model = build_ngram_model(full_text, n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BOS> <BOS>\n",
      "\t\"\t0.8073\n",
      "\tThe\t0.0304\n",
      "\tHolmes\t0.0230\n",
      "\tI\t0.0167\n",
      "\tIt\t0.0141\n",
      "\t[206 more...]\n",
      "<BOS> It\n",
      "\twas\t0.8235\n",
      "\tis\t0.0515\n",
      "\tmay\t0.0221\n",
      "\thad\t0.0147\n",
      "\tseemed\t0.0074\n",
      "\t[11 more...]\n",
      "It was\n",
      "\ta\t0.1888\n",
      "\tthe\t0.0686\n",
      "\tnot\t0.0562\n",
      "\tonly\t0.0312\n",
      "\tan\t0.0250\n",
      "\t[184 more...]\n",
      "my dear\n",
      "\tWatson\t0.5612\n",
      "\tfellow\t0.1429\n",
      "\tsir\t0.0918\n",
      "\tyoung\t0.0510\n",
      "\tVon\t0.0204\n",
      "\t[12 more...]\n"
     ]
    }
   ],
   "source": [
    "for prefix in [(BOS, BOS), (BOS, 'It'), ('It', 'was'), ('my', 'dear')]:\n",
    "    print(*prefix)\n",
    "    sorted_probs = sorted(model[prefix].items(), key=lambda x: -x[1])\n",
    "    for k, v in sorted_probs[:5]:\n",
    "        print(f'\\t{k}\\t{v:.4f}')\n",
    "    print(f'\\t[{len(sorted_probs)-5} more...]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do these probabilities look reasonable? How can we systematically evaluate the quality of our model? We'll cover this in the next lesson!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional Extra:\n",
    " - Smoothing techniques, such as [Laplace Smoothing](https://en.wikipedia.org/wiki/Additive_smoothing), are often added to n-gram language models to deal with probabilities of 0. How might you modify your code to include smoothing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ngram_model(texts: List[List[str]], n: int) -> Dict[Tuple[str, ...], Dict[str, float]]:\n",
    "    ngram_counts = count_ngrams(texts, n)\n",
    "    ngram_model = {}\n",
    "\n",
    "    # Build a set of all unique tokens in the texts\n",
    "    all_tokens = set()\n",
    "    for tokens in texts:\n",
    "        all_tokens.update(tokens)\n",
    "    \n",
    "    # Add BOS and EOS tokens to the set of all unique tokens\n",
    "    all_tokens.update([BOS, EOS])\n",
    "    V = len(all_tokens)  # Vocabulary size\n",
    "\n",
    "    for prefix, suffix_counts in ngram_counts.items():\n",
    "        total_count = sum(suffix_counts.values()) + V  # Adjust total count for smoothing\n",
    "        ngram_model[prefix] = {suffix: (count + 1) / total_count for suffix, count in suffix_counts.items()}\n",
    "\n",
    "        # Add probabilities for unseen suffixes\n",
    "        for token in all_tokens:\n",
    "            if token not in ngram_model[prefix]:\n",
    "                ngram_model[prefix][token] = 1 / total_count\n",
    "\n",
    "    return ngram_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
