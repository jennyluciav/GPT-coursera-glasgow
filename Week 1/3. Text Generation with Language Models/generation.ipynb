{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3: Text Generation with an N-Gram Language Model\n",
    "\n",
    "In this lab, you will implement two text generation strategies covered in lecture: Greedy Search and Sampling.\n",
    "\n",
    "Here's a revision of the n-gram language model implementation from the previous lab. It now includes a `get_prob_dist()` function, which returns the probabilities of all tokens given the context.\n",
    "\n",
    "Look over the implementation to be sure that you understand it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "BOS = '<BOS>'\n",
    "EOS = '<EOS>'\n",
    "OOV = '<OOV>'\n",
    "class NGramLM:\n",
    "    def __init__(self, path, smoothing=0.001, verbose=False):\n",
    "        with open(path, 'rb') as fin:\n",
    "            data = pickle.load(fin)\n",
    "        self.n = data['n']\n",
    "        self.V = set(data['V'])\n",
    "        self.model = data['model']\n",
    "        self.smoothing = smoothing\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def get_prob_dist(self, context):\n",
    "        # Take only the n-1 most recent context (Markov Assumption)\n",
    "        context = tuple(context[-self.n+1:])\n",
    "        # Add <BOS> tokens if the context is too short, i.e., it's at the start of the sequence\n",
    "        while len(context) < (self.n-1):\n",
    "            context = (BOS,) + context\n",
    "        # Handle words that were not encountered during the training by replacing them with a special <OOV> token\n",
    "        context = tuple((c if c in self.V else OOV) for c in context)\n",
    "        if context in self.model:\n",
    "            # Compute the probability distribution using a Maximum Likelihood Estimation and Laplace Smoothing\n",
    "            norm = sum(self.model[context].values()) + self.smoothing * len(self.V)\n",
    "            prob_dist = {k: (c + self.smoothing) / norm for k, c in self.model[context].items()}\n",
    "            for word in self.V - prob_dist.keys():\n",
    "                prob_dist[word] = self.smoothing / norm\n",
    "        else:\n",
    "            # Simplified formula if we never encountered this context; the probability of all tokens is uniform\n",
    "            prob = 1 / len(self.V)\n",
    "            prob_dist = {k: prob for k in self.V}\n",
    "        prob_dist = dict(sorted(prob_dist.items(), key=lambda x: (-x[1], x[0])))\n",
    "        return prob_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-built n-gram languae models\n",
    "model_unigram = NGramLM('arthur-conan-doyle.tok.train.n1.pkl')\n",
    "model_bigram = NGramLM('arthur-conan-doyle.tok.train.n2.pkl')\n",
    "model_trigram = NGramLM('arthur-conan-doyle.tok.train.n3.pkl')\n",
    "model_4gram = NGramLM('arthur-conan-doyle.tok.train.n4.pkl')\n",
    "model_5gram = NGramLM('arthur-conan-doyle.tok.train.n5.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at some of the probability distributions.\n",
    "\n",
    "Are they reasonable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bigram.get_prob_dist(['my'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bigram.get_prob_dist(['.'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trigram.get_prob_dist(['my', 'dear'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now we have all the tools we need to start generating text!\n",
    "\n",
    "We'll start with a simple greedy generation approach. Your task is to implement greedy generation below.\n",
    "\n",
    "Note: we have a `max_length` parameter to be sure that the generation process doesn't go on forever. You can stop when your sequence either reaches an `<EOS>` token or is the maximum length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "def greedy_generation(model: NGramLM, context: List[str], max_length: int = 100) -> List[str]:\n",
    "    pass\n",
    "    # your code here\n",
    "\n",
    "greedy_generation(model_trigram, ['\"\"', 'My', 'dear', 'Watson'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! How does the generation look? Feel free to try out a several samples below.\n",
    "\n",
    "Is it deterministic? Are the generated sequences interesting?\n",
    "\n",
    "Consider trying different model types. What are the different qualities that you see from the unigram, bigram, trigram, 4-gram, and 5-gram models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to implement sampling.\n",
    "\n",
    "We now include a `topk` argument. This reduces the candidate set of probabilities down to only the `topk` highest-probability items. This helps reduce the chance of generating highly unlikely sequences.\n",
    "\n",
    "Note: consider using [`random.choices`](https://docs.python.org/3/library/random.html#random.choices) to help in sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import random\n",
    "def sampling_generation(model: NGramLM, context: List[str], max_length: int = 100, topk=10) -> List[str]:\n",
    "    pass\n",
    "    # your code here\n",
    "\n",
    "sampling_generation(model_trigram, ['\"\"', 'My', 'dear', 'Watson'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now qualitatively compare your sampling generation with the greedy generation.\n",
    "\n",
    "What do you notice about the generated sequences? How do models of different sizes behave? What is the effect of `topk`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional Extras:\n",
    " - Try implementing a beam search strategy. Does it tend to lead to qualitatively better results than the other two approaches?\n",
    " - What strategy might you take to efficiently find the most likely sequence for an n-gram language model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
