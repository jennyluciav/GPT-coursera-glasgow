# Generative Pre-trained Transformers (GPT)
This repository contains all the resources for the labs in [Generative Pre-trained Transformers (GPT)](https://www.coursera.org/learn/chatgpt) course on Coursera

# Course Syllabus
## [Week 1 - Language Modeling](/Week%201/)
* What is a Language Model?
* N-gram Language Models
* Evaluating Language Models
* Generating Text

## Week 2 - Transformers and GPT
* Representing text with numerical vectors​
* Words, tokens or sub-tokens​
* Neural language models
* The Transformer architectures
* How are Transformers trained?
* GPT and BERT
* Using GPT
* The ethical challenges of large language models

## Week 3 - Applications and Implications
* Hallucinations in LLMs
* Chatbots, RLHF, and ChatGPT
* Academic conduct
* Creativity and copyright
* Regulation and risks
